\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage[font={small,it}]{caption}
\usepackage{float}
\usepackage{setspace}

\hypersetup{
  colorlinks   = true,  %Colours links instead of ugly boxes
  urlcolor     = blue,  %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   =  blue   %Colour of citations
}

\begin{document}
\title{Green Functions: The Bridge Between Linear Algebra and Differential Equations}
\author{Andrew Carnes}

\maketitle

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION} \label{intro}

In this document, we look at differential equations as linear algebra problems, and we look at the Green function as the inverse of a differential operator. Consider the linear algebra problem $M\vec{v}=\vec{b}$ with the matrix $M$ and the vectors $\vec{v}$ and $\vec{b}$. Finding $M^{-1}$ solves for $\vec{v}$ via $\vec{v}=M^{-1}b$. In the same way, the Green function $G=D^{-1}$ solves the differential equation $Df=g$, where D is some differential operator and f and g are functions. In order to view the Green function this way, we develop a corresopndence between matrices and derivatives and functions and vectors. The note starts by extrapolating dot products from vectors to functions, then uses dot products to develop matrix multiplication on functions, and then uses the matrix-function framework to make the connection between linear algebra and Diff EQ. Looking at Diff EQ in this light, we find that the Green function is the inverse of a differential operator. 

The journey from vectors to functions and linear algebra to Diff EQ provides some intersting insight into some unsuspecting places like the Dirac delta function, the fundamental theorem of calculus, and Fourier transforms. Nothing here is super rigorous, but the document should get the main points across and provide a new way to look at some familiar things. Anywho, let's get on with the adventure.

%%%%%%%%%%%%%%%%%%%% FUNCTIONS AS VECTORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FUNCTIONS AS VECTORS}  

This section compares functions to vectors and extrapolates the familiar vector dot product to functions, providing the foundation of the framework that takes us from matrices and vectors to functions and Diff EQ. Onwards! What is a vector? A vector $\vec{v}$ is an ordered 1D storage device. A vector of length N has N bins to put numbers in, and you keep track of the bins by labeling them with an integer. Note that you can only put one number in each bin. If we label the bins with i, $v_i$ is the information in the i$^{th}$ bin. $\vec{v}$ usually written like so, 

\begin{equation}
\vec{v} = 
\begin{pmatrix}
v_1 \\
v_2 \\
... \\
v_N \\
\end{pmatrix}.
\end{equation}
Now consider a function. A function f(x) is an ordered 1D storage device with a bunch of bins. The bins are labeled by x, and f(x) is the number in the x$^{th}$ bin. Each bin only has one number. In this case, x is a real number so f(x) is much more dense than $\vec{v}$. f(x) can be written like so,  
\begin{equation}
f(x) = 
\begin{pmatrix}
f(-\infty) \\
... \\
f(-3.11111111128) \\
... \\
f(-3.1111111112) \\
... \\
f(0) \\
... \\
f(912.99) \\
... \\
f(999999999) \\
... \\
f(\infty) \\
\end{pmatrix}.
\end{equation}
Between any two bins there are an infinite number of bins (welcome to the real numbers), which makes things tricky, but aside from the storage density, functions and vectors work the same way. Give them a location and they spit out the info you want. 

Let's discretize the real number line into chunks of width dx. We can then label the entries in f(x) by f($x_i$), where $x_{i+1} - x_i = dx$. Then we have  
\begin{equation}
f(x) = 
\begin{pmatrix}
f(x_{-\infty}) \\
... \\
... \\
f(x_{-2}) \\
... \\
f(x_{-1}) \\
... \\
f(x_0) \\
... \\
f(x_1) \\
... \\
f(x_2) \\
... \\
... \\
f(x{\infty}) \\
\end{pmatrix}.
\end{equation}
Note that there are still an infinite number of entries between f($x_i$) and f($x_{i+1}$). Now let's take dot products. With two vectors, $\vec{v}$ and $\vec{u}$, the dot product is 
\begin{equation}
\vec{v} \cdot \vec{u} = v_1 u_1 + v_2 u_2 + ... v_N u_N = \sum_{i=1}^{N} v_i u_i.
\end{equation}
As usual, when $\vec{v} \cdot \vec{u} = 0$ the vectors are orthogonal, and when $\frac{\vec{v} \cdot \vec{u}}{\sqrt{v^2u^2}} = 0$ the vectors are parallel. Moreover, when all $v_i$ and $u_i$ are 1, $\vec{v} \cdot \vec{u} = N$, the total number of components. If we consider only a subset of components with all $v_i = u_i = 1$ then $\sum_{i=1}^{N_c} v_i u_i = N_c$, the number of components summed over. 

Now take a dot product of functions f(x) and g(x), 
\begin{equation}
\begin{split}
f(x) \cdot g(x) = & ... + f(x_{-2})g(x_{-2}) + ... + f(x_{-1})g(x_{-1}) \\ 
&+ ... + f(x_0)g(x_0) + ... + f(x_1)g(x_1) + ... + f(x_2)g(x_2) + ...
\end{split}
\end{equation}
Let's look at the sum from $x_1$ to $x_{2}$ when f(x) = g(x) = 1 in all bins 
\begin{equation}
f(x) \cdot g(x) = f(x_1)g(x_1) + ... + ... \text{ (up to } f(x_2)g(x_2)) = 1 + 1 + ... + 1 = N_c.
\end{equation}
This gives us the number of components summed over, $N_c$, which is infinite. That's true, but it's not a helpful normalization. Let's redefine the dot product between functions to yield the size of the real interval summed over, $x_2 - x_1 = dx$. 
\begin{equation}
\begin{split}
f(x) \cdot g(x) &= f(x_1)g(x_1)\epsilon + ... + ... \text{ (up to } f(x_2)g(x_2)) \\ 
& = 1\epsilon + ... + 1\epsilon = N_c\epsilon = dx.
\end{split}
\end{equation}
The full dot product is now
\begin{equation}
\begin{split}
f(x) \cdot g(x) = &... + f(x_{-2})g(x_{-2})\epsilon + ... + f(x_{-1})g(x_{-1})\epsilon \\ 
&+ ... + f(x_0)g(x_0)\epsilon + ... + f(x_1)g(x_1)\epsilon + ... + f(x_2)g(x_2)\epsilon + ...
\end{split}
\end{equation}
Finally, let's represent all $N_c$ terms between f($x_i$)g($x_i$) and f($x_{i+1}$)g($x_{i+1}$) by f($x_i$)g($x_i$) using the assumption that f(x) and g(x) change very little throughout the interval dx. This yields 
\begin{equation}
\begin{split}
f(x) \cdot g(x) &= ... + f(x_{-2})g(x_{-2})N_c\epsilon + f(x_{-1})g(x_{-1})N_c\epsilon + f(x_0)g(x_0)N_c\epsilon \\ 
&+ f(x_1)g(x_1)N_c\epsilon + f(x_2)g(x_2)N_c\epsilon + ... \\ 
\end{split}
\end{equation}
Remembering that $N_c\epsilon = dx$, we get
\begin{equation}
f(x) \cdot g(x) = \sum_{i=-\infty}^{\infty} f(x_i)g(x_i) dx = \int_{-\infty}^{\infty} f(x)g(x)dx.
\end{equation}
As with vectors, when $f(x) \cdot g(x) = 0$, two functions are orthogonal. 

Now that we have dot products between functions down, we can develop matrix multiplication on functions. We just view matrix multiplication in terms of dot products, and BAM we've got ourselves some wacky version of linear algebra. Let's move on to this next part. 
% consider talking about plotting vectors vs functions. vectors as arrows, functions as arrows or vectors as plots.


%%%%%%%%%%%%%%%%%%%% MATRICES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FROM MATRICES TO 2D FUNCTIONS}
This section develops matrix multiplication on functions in terms of dot products. Once we have this down, we can move on to write the derivative as a matrix and develop the connection between linear algebra and Diff EQ. OK so, let's figure out this matrix-function business. 

A matrix transforms a vector into another vector. Consider a rotation,
\begin{equation}
R\vec{v} = 
\begin{pmatrix}
cos\theta  & sin\theta \\
-sin\theta & cos\theta \\
\end{pmatrix}
\begin{pmatrix}
v_1 \\
v_2 \\
\end{pmatrix}
= 
\begin{pmatrix}
v_1cos\theta  + v_2sin\theta \\
-v_1sin\theta + v_2cos\theta \\
\end{pmatrix}
= \vec{v'}.
\end{equation}
The vector $\vec{v'}$ is still a vector, it's just $\vec{v}$ rotated by $\theta$. We can write the matrix multiplication in summation form like so $v'_i = \sum_j R_{ij}v_j$. We can also write the transformation in terms of dot products $v'_i = \vec{R}_i \cdot \vec{v}$, where $\vec{R}_i$ is the row vector in row i of the rotation matrix R. 

We can use the function dot product to extend this to functions, $g(x) = K(x) \cdot f$, where K(x) is the $x^th$ row of the matrix K. Writing out the dot product in terms of an integral yields $\int K(x,y)f(y)dy$. Integrating f(y) against the 2D function K(x,y) performs the matrix multiplication yielding the new function g(x). Let's write $Kf = g$ to represent this operation. Imagine the discretized matrix multiplication,
\begin{equation}
\begin{split}
&\begin{pmatrix}
K(y_1,x_1) & K(y_1, x_2) & ... & K(y_1, x_N) \\
K(y_2,x_1) & K(y_2, x_2) & ... & K(y_2, x_N) \\
... & ... & ... & ... \\
K(y_N,x_1) & K(y_N, x_2) & ... & K(y_N, x_N) \\
\end{pmatrix}
\begin{pmatrix}
f(x_1) \\
f(x_2) \\
... \\ 
f(x_N) \\
\end{pmatrix} \\
&=
\begin{pmatrix}
K(y_1,x_1)f(x_1)dx + K(y_1, x_2)f(x_2)dx + ... + K(y_1, x_N)f(x_N) \\
K(y_2,x_1)f(x_1)dx + K(y_2, x_2)f(x_2)dx + ... + K(y_2, x_N)f(x_N) \\
... + ... + ... + ... \\
K(y_N,x_1)f(x_1)dx + K(y_N, x_2)f(x_2)dx + ... + K(y_N, x_N)f(x_N) \\
\end{pmatrix}
=
\begin{pmatrix}
g(y_1) \\
g(y_2) \\
...    \\
g(y_N) \\
\end{pmatrix}.
\end{split}
\end{equation}
An example is the Fourier transform,
\begin{equation}
F(k,x)f(x) = \frac{1}{2\pi}\int e^{-ikx}f(x)dx = g(k), 
\end{equation} 
which we will come back to later.

%%%%%%%%%%%%%%%%%%%% DERIVATIVE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{THE DERIVATIVE AS A MATRIX}
Just like a matrix turns a vector into another vector, the derivative turns a function into another function, $\frac{d}{dx}f(x) = \frac{f(x+dx) - f(x)}{dx} = f'(x)$. If we can understand the derivative as a matrix, we can turn differential equations (Diff EQ) problems into linear algebra problems and solve them using the same techniques. 

Let's write $\frac{d}{dx}f(x)$ in our matrix multiplication $Df = \int D(y,x)f(x)dx = g(y)$ formalism,
\begin{equation}
\begin{split}
Df &= \frac{1}{dxdx} 
\begin{pmatrix}
... & ... & ... & ... & ... & ... & ... & ... \\
... & -1 & 1 & 0 & 0 & 0 & 0 & ...\\
... & 0 & -1 & 1 & 0 & 0 & 0 & ... \\
... & 0 & 0 & -1 & 1 & 0 & 0 & ... \\
... & 0 & 0 & 0 & -1 & 1 & 0 & ... \\
... & ... & ... & ... & ... & ... & ... & ... \\
\end{pmatrix}
\begin{pmatrix}
... \\
f(x_i) \\
f(x_{i+1}) \\
f(x_{i+2}) \\
f(x_{i+3}) \\
f(x_{i+4}) \\
f(x_{i+5}) \\
... \\
\end{pmatrix}
=
\begin{pmatrix}
... \\
\frac{-f(x_i) + f(x_{i+1})}{dxdx}dx \\
\frac{-f(x_{i+1}) + f(x_{i+2})}{dxdx}dx \\
\frac{-f(x_{i+2}) + f(x_{i+3})}{dxdx}dx \\
\frac{-f(x_{i+3}) + f(x_{i+4})}{dxdx}dx \\
\frac{-f(x_{i+4}) + f(x_{i+5})}{dxdx}dx \\
\frac{-f(x_{i+5}) + f(x_{i+6})}{dxdx}dx \\
... \\
\end{pmatrix}\\
&=
\begin{pmatrix}
... \\
\frac{f(x_{i+1}) - f(x_i) }{dx} \\
\frac{f(x_{i+2}) - f(x_{i+1})}{dx} \\
\frac{f(x_{i+3}) - f(x_{i+2})}{dx} \\
\frac{f(x_{i+4}) - f(x_{i+3})}{dx} \\
\frac{f(x_{i+5}) - f(x_{i+4})}{dx} \\
\frac{f(x_{i+6}) - f(x_{i+5})}{dx} \\
... \\
\end{pmatrix}
=
\begin{pmatrix}
... \\
g(y_i) \\
g(y_{i+1}) \\
g(y_{i+2}) \\
g(y_{i+3}) \\
g(y_{i+4}) \\
g(y_{i+6}) \\
... \\
\end{pmatrix}
\end{split}
\end{equation}
Note that an integral multiplies by a factor of dx, and a derivative divides by dx. We need $\frac{d}{dx}f = \int D(y,x)f(x)dx$ with dimensions like $\frac{1}{dx} \sim D dx$; therefore, $D \sim \frac{1}{dxdx}$.

So it turns out you can pretty much just write down the derivative in matrix form. How nice. Let's use this to check out the fundamental theorem of calculus, and then return to the whole differential equations are just linear algebra business. 

%%%%%%%%%%%%%%%%%%%% THE FUNDAMENTAL THEOREM OF CALCULUS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{THE FUNDAMENTAL THEOREM OF CALCULUS}

The matrixified derivative provides some interesting insight into a familiar theorem of calculus. If I remember right, the fundamental theorem of calculus is $\int_a^b \frac{d}{dx}f(x)dx = f(a) - f(b)$. Let's see what happens if we integrate $\frac{d}{dx}f(x)$.
\begin{equation}
\begin{split}
\int_a^b f'(x)dx &= 1 \cdot f'(x) = 
dx
\begin{pmatrix}
... & 1 & 1 & 1 & 1 & 1 & 1 & ... 
\end{pmatrix}
\begin{pmatrix}
... \\
\frac{f(x_{i+1}) - f(x_i) }{dx} \\
\frac{f(x_{i+2}) - f(x_{i+1})}{dx} \\
\frac{f(x_{i+3}) - f(x_{i+2})}{dx} \\
\frac{f(x_{i+4}) - f(x_{i+3})}{dx} \\
\frac{f(x_{i+5}) - f(x_{i+4})}{dx} \\
\frac{f(x_{i+6}) - f(x_{i+5})}{dx} \\
... \\
\end{pmatrix}\\
&= f(b) + ... + f(x_{i+6}) - f(x_{i+5}) + f(x_{i+5}) - f(x_{i+4}) + f(x_{i+4}) - f(x_{i+3}) + ... - f(a) \\ 
&= f(b) - f(a) 
\end{split}
\end{equation}
Boom! The fundamental theorem of calculus. Nailed it. 

\section{LINEAR ALGEBRA IN A NUTSHELL}
Now onto linear algebra. If the derivative is a matrix, then any differential equation is just some linear algebra problem. Let's figure out a general method to solve some linear algebra mystery, and then we can use the same strategy for Diff EQ. With a linear algebra problem like $M\vec{v} = \vec{b}$, we can solve for $\vec{v}$ by finding $M^{-1}$. With $M^{-1}$ in hand, $\vec{v} = M^{-1}\vec{b}$. Problem solved. But how do we find $M^{-1}$? Well, $M^{-1}$ is easy to find when $M$ is diagonal. Let's go to the bar basis where M is diagonal.
\begin{equation}
\label{minv}
\begin{split}
\bar{M}^{-1}\bar{M} &= 1 = 
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & 1/m_i & 0 & 0 & ... \\
... & 0 & 1/m_{i+1} & 0 & ... \\
... & 0 & 0 & 1/m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix}
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & m_i & 0 & 0 & ... \\
... & 0 & m_{i+1} & 0 & ... \\
... & 0 & 0 & m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix} \\
&\rightarrow \bar{M}^{-1} =  
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & 1/m_i & 0 & 0 & ... \\
... & 0 & 1/m_{i+1} & 0 & ... \\
... & 0 & 0 & 1/m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix}
\end{split}
\end{equation}
In the bar basis, $\bar{M}$ consists of its eigenvalues along the diagonal, and $\bar{M}^{-1}$ is just one over the eigenvalues along the diagonal. The equation to solve becomes $\vec{\bar{v}} = \bar{M}^{-1}\vec{\bar{b}}$. If we assume there is some transformation matrix T that transforms the non-bar basis to the bar basis, then $T\vec{v} = \bar{M}^{-1}T\vec{b}$ and $\vec{v} = T^{-1}\bar{M}^{-1}T\vec{b}$. Finally, 
\begin{equation}
M^{-1} = T^{-1}\bar{M}^{-1}T.   
\end{equation}
Almost there, we still have to find $T$ and $T^{-1}$! $\vec{\bar{m}}_{i}$, the eigenvector in the bar basis corresponding to eigenvalue $m_i$, is just $\begin{pmatrix}... & 0 & 1 & 0 ... \end{pmatrix}$, where the 1 is in the $i^{th}$ position. $T^{-1}$ converts from the bar basis to the non-bar basis and should convert $\vec{\bar{m}}_{i}$ to $\vec{m}_{i}$. This holds for all eigenvectors. Naturally,
\begin{equation}
\label{tinv}
T^{-1}\vec{\bar{m}}_{i} = \vec{m}_{i} \rightarrow T^{-1} = 
\left( \begin{array}{c|c|c|c|c|c}
   \vec{m}_1 & \vec{m}_2 & ... & \vec{m}_i & ... & \vec{m}_N 
\end{array}\right)
\end{equation} 
Note that $\vec{m}_i$ defines the entire $i^{th}$ column of the $T^{-1}$ matrix. Checking this,
\begin{equation}
T^{-1}\vec{\bar{m}}_{i} = 
\left( \begin{array}{c|c|c|c|c|c}
   \vec{m}_1 & \vec{m}_2 & ... & \vec{m}_i & ... & \vec{m}_N 
\end{array}\right)
\begin{pmatrix}
0 \\ 0 \\ ... \\ 1 \\ ... \\ 0 \\ 
\end{pmatrix}
= 0\vec{v}_{1} + 0 \vec{v}_{2} + ... + \vec{v}_{i} + ... + 0\vec{v}_{N} = \vec{v}_{i}.
\end{equation} 
T is built from orthonormal eigenvectors which implies that $T^{\dagger}T = 1$, where $\dagger$ represents the Hermitian conjugate. 
\begin{equation}
T^{\dagger}T =  
\begin{pmatrix}
\vec{v}_1^\dagger \vec{v}_1 & \vec{v}_1^\dagger \vec{v}_2 & ... & \vec{v}_1^\dagger \vec{v}_N \\
\vec{v}_2^\dagger \vec{v}_1 & \vec{v}_2^\dagger \vec{v}_2 & ... & \vec{v}_2^\dagger \vec{v}_N \\
... & ... & ... & ... \\
\vec{v}_N^\dagger \vec{v}_1 & \vec{v}_N^\dagger \vec{v}_2 & ... & \vec{v}_N^\dagger \vec{v}_N \\
\end{pmatrix}
= 1
\end{equation}
$T^{\dagger}T = 1$ further implies that $T^{-1} = T^{\dagger}$ and $T = (T^{-1})^{\dagger}$. We now have the entire solution in hand. $M^{-1} = T^{-1}\bar{M}^{-1}T = T^{\dagger}\bar{M}^{-1}T$ with $\bar{M}$ given by one over the eigenvalues of $M$ (Equation \ref{minv}) and $T^{-1} = T^\dagger$ given by the eigenvectors of $M$ (Equation \ref{tinv}).

Let's go through a simple example to solidify things. Consider the linear algebra problem $M\vec{v} = \vec{b}$ with 
\begin{equation} 
M = 
\begin{pmatrix}
0 & 1 \\
-1 & 0 \\
\end{pmatrix},
\vec{b} = 
\begin{pmatrix}
1 & 0 \\
\end{pmatrix},
\end{equation}
and $\vec{v}$ unknown. To solve for $\vec{v}$, we find $M^{-1}$ following the process described above. In the eigenbasis, $M$ is diagonal and represented by $\bar{M}$,
\begin{equation}
\bar{M} = 
\begin{pmatrix}
m_1 & 0 \\
0 & m_2 \\ 
\end{pmatrix}.
\end{equation}
$\bar{M}^{-1}$ is then 
\begin{equation}
\bar{M} = 
\begin{pmatrix}
1/m_1 & 0 \\
0 & 1/m_2 \\ 
\end{pmatrix}.
\end{equation}
Finally, $M^{-1} = T^{-1}\bar{M}^{-1}T$, where $T$ converts from the non-bar to the bar basis, and $T^{-1}$ converts from the bar basis to the non-bar basis. $T^{-1}$ is built from the eigenvectors of M, $\vec{m}_1$ and $\vec{m}_2$. Now we just need the eigenvalues and eigenvectors. We use the canonical equation $det[M - 1m_i] = 0$ to get the eigenvalues, 
\begin{equation}
det[M] = det\left[
\begin{pmatrix}
m_i & 1 \\
-1 & m_i \\ 
\end{pmatrix}\right] = m_i^2 + 1 = 0.
\end{equation}
This yields $m_1 = i$ and $m_2 = -i$. Now onto the eigenvectors,
\begin{equation}
M\vec{m}_i = m_i\vec{m}_i =
\begin{pmatrix}
0 & 1 \\
-1 & 0 \\ 
\end{pmatrix} 
\begin{pmatrix}
m_{i1} \\ m_{i2} \\
\end{pmatrix}
=
m_i
\begin{pmatrix}
m_{i1} \\ m_{i2} \\
\end{pmatrix}. 
\end{equation}
Plugging in $m_1=i$ provides $im_{11} = m_{12}$, while $m_2=-i$ provides $-im_{21} = m_{22}$. Choosing $m_{11} = 1$ and $m_{21} = 1$ sets the eigenvectors to 
\begin{equation}
\vec{m}_1 = 
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 \\ i \\
\end{pmatrix}
\text{ and }
\vec{m}_2 = 
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 \\ -i \\
\end{pmatrix}.
\end{equation}
$T^{-1}$ is then given by 
\begin{equation}
T^{-1} =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1 \\
i & -i \\
\end{pmatrix}.
\end{equation}
And the Hermitian conjugate of $T^{-1}$ provides $T$,
\begin{equation}
T =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & -i \\
1 & i \\
\end{pmatrix}.
\end{equation}
Putting everything together,
\begin{equation}
M^{-1} = T^{-1}\bar{M}^{-1}T =
\frac{1}{2}
\begin{pmatrix}
1 & 1 \\
i & -i \\
\end{pmatrix}
\begin{pmatrix}
-i & 0 \\
0 & i \\
\end{pmatrix}
\begin{pmatrix}
1 & -i \\
1 & i \\
\end{pmatrix}
=
\begin{pmatrix}
0 & -1 \\
1 & 0 \\
\end{pmatrix}.
\end{equation}
Now we use the inverse matrix to solve the linear algebra problem,
\begin{equation}
\begin{pmatrix}
0 & -1 \\
1 & 0 \\
\end{pmatrix}
\begin{pmatrix}
1 \\
0 \\
\end{pmatrix}
=
\begin{pmatrix}
0 \\
1 \\
\end{pmatrix}.
\end{equation}
Finally, we find that $\vec{v} = \begin{pmatrix} 1 \\ 0 \\ \end{pmatrix}$, solving our linear algebra problem.

In this section, we learned a general method to solve $M\vec{v}=\vec{b}$. We built $M^{-1}$ using the eigenvalues and eigenvectors of $M$ and used $M^{-1}$ to solve for $\vec{v}$. In the next section, we will solve the differential equation $Df=g$, where D is a differential operator and f and g are functions. We will build $D^{-1}$ from the eigenvalues and eigenfunctions of D and solve the differential equation for f. 
\end{document}
