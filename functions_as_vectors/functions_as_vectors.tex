\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage[font={small,it}]{caption}
\usepackage{float}
\usepackage{setspace}

\hypersetup{
  colorlinks   = true,  %Colours links instead of ugly boxes
  urlcolor     = blue,  %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   =  blue   %Colour of citations
}

\begin{document}
\title{Functions, Vectors, Operators, Matrices, and Green Functions}
\author{Andrew Carnes}

\maketitle

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION} \label{intro}

This document looks at functions as vectors and reinterprets derivatives, Green functions, and the Dirac delta function as matrices. We also look at dot products, integrals, and transformations (like the Fourier transform) in this formalism. We find that the Green function for a differential operator is the inverse matrix of that differential operator. Looking at Green functions this way really helped me with electromagnetism and QFT in graduate school. Plus it's pretty cool. Nothing here is super rigorous, but it gets the point across and provides a new way to look at things.

%%%%%%%%%%%%%%%%%%%% FUNCTIONS AS VECTORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FUNCTIONS AS VECTORS}  

What is a vector? A vector $\vec{v}$ is an ordered 1D storage device. A vector of length N has N bins to put numbers in, and you keep track of the bins by labeling them with an integer. Note that you can only put one number in each bin. If we label the bins with i, $v_i$ is the information in the i$^{th}$ bin. $\vec{v}$ usually written like so, 

\begin{equation}
\vec{v} = 
\begin{pmatrix}
v_1 \\
v_2 \\
... \\
v_N \\
\end{pmatrix}.
\end{equation}
Now consider a function. A function f(x) is an ordered 1D storage device with a bunch of bins. The bins are labeled by x, and f(x) is the number in the x$^{th}$ bin. Each bin only has one number. In this case, x is a real number so f(x) is much more dense than $\vec{v}$. f(x) can be written like so,  
\begin{equation}
f(x) = 
\begin{pmatrix}
f(-\infty) \\
... \\
f(-3.11111111128) \\
... \\
f(-3.1111111112) \\
... \\
f(0) \\
... \\
f(912.99) \\
... \\
f(999999999) \\
... \\
f(\infty) \\
\end{pmatrix}.
\end{equation}
Between any two bins there are an infinite number of bins (welcome to the real numbers), which makes things tricky, but aside from the storage density, functions and vectors work the same way. Give them a location and they spit out the info you want. 

Let's discretize the real number line into chunks of width dx. We can then label the entries in f(x) by f($x_i$), where $x_{i+1} - x_i = dx$. Then we have  
\begin{equation}
f(x) = 
\begin{pmatrix}
f(x_{-\infty}) \\
... \\
... \\
f(x_{-2}) \\
... \\
f(x_{-1}) \\
... \\
f(x_0) \\
... \\
f(x_1) \\
... \\
f(x_2) \\
... \\
... \\
f(x{\infty}) \\
\end{pmatrix}.
\end{equation}
Note that there are still an infinite number of entries between f($x_i$) and f($x_{i+1}$). Now let's take dot products. With two vectors, $\vec{v}$ and $\vec{u}$, the dot product is 
\begin{equation}
\vec{v} \cdot \vec{u} = v_1 u_1 + v_2 u_2 + ... v_N u_N = \sum_{i=1}^{N} v_i u_i.
\end{equation}
As usual, when $\vec{v} \cdot \vec{u} = 0$ the vectors are orthogonal, and when $\frac{\vec{v} \cdot \vec{u}}{\sqrt{v^2u^2}} = 0$ the vectors are parallel. Moreover, when all $v_i$ and $u_i$ are 1, $\vec{v} \cdot \vec{u} = N$, the total number of components. If we consider only a subset of components with all $v_i = u_i = 1$ then $\sum_{i=1}^{N_c} v_i u_i = N_c$, the number of components summed over. 

Now take a dot product of functions f(x) and g(x), 
\begin{equation}
\begin{split}
f(x) \cdot g(x) = & ... + f(x_{-2})g(x_{-2}) + ... + f(x_{-1})g(x_{-1}) \\ 
&+ ... + f(x_0)g(x_0) + ... + f(x_1)g(x_1) + ... + f(x_2)g(x_2) + ...
\end{split}
\end{equation}
Let's look at the sum from $x_1$ to $x_{2}$ when f(x) = g(x) = 1 in all bins 
\begin{equation}
f(x) \cdot g(x) = f(x_1)g(x_1) + ... + ... \text{ (up to } f(x_2)g(x_2)) = 1 + 1 + ... + 1 = N_c.
\end{equation}
This gives us the number of components summed over, $N_c$, which is infinite. That's true, but it's not a helpful normalization. Let's redefine the dot product between functions to yield the size of the real interval summed over, $x_2 - x_1 = dx$. 
\begin{equation}
\begin{split}
f(x) \cdot g(x) &= f(x_1)g(x_1)\epsilon + ... + ... \text{ (up to } f(x_2)g(x_2)) \\ 
& = 1\epsilon + ... + 1\epsilon = N_c\epsilon = dx.
\end{split}
\end{equation}
The full dot product is now
\begin{equation}
\begin{split}
f(x) \cdot g(x) = &... + f(x_{-2})g(x_{-2})\epsilon + ... + f(x_{-1})g(x_{-1})\epsilon \\ 
&+ ... + f(x_0)g(x_0)\epsilon + ... + f(x_1)g(x_1)\epsilon + ... + f(x_2)g(x_2)\epsilon + ...
\end{split}
\end{equation}
Finally, let's represent all $N_c$ terms between f($x_i$)g($x_i$) and f($x_{i+1}$)g($x_{i+1}$) by f($x_i$)g($x_i$) using the assumption that f(x) and g(x) change very little throughout the interval dx. This yields 
\begin{equation}
\begin{split}
f(x) \cdot g(x) &= ... + f(x_{-2})g(x_{-2})N_c\epsilon + f(x_{-1})g(x_{-1})N_c\epsilon + f(x_0)g(x_0)N_c\epsilon \\ 
&+ f(x_1)g(x_1)N_c\epsilon + f(x_2)g(x_2)N_c\epsilon + ... \\ 
\end{split}
\end{equation}
Remembering that $N_c\epsilon = dx$, we get
\begin{equation}
f(x) \cdot g(x) = \sum_{i=-\infty}^{\infty} f(x_i)g(x_i) dx = \int_{-\infty}^{\infty} f(x)g(x)dx.
\end{equation}
When $f(x) \cdot g(x) = 0$, two functions are orthogonal.
% consider talking about plotting vectors vs functions. vectors as arrows, functions as arrows or vectors as plots.


%%%%%%%%%%%%%%%%%%%% DISCRETIZING LINEAR TRANSFORMATIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DISCRETIZING LINEAR TRANSFORMATIONS}  

%%%%%%%%%%%%%%%%%%%% MATRICES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Continuous Linear Transformations as Matrices}
A matrix transforms a vector into another vector. Consider a rotation,
\begin{equation}
R\vec{v} = 
\begin{pmatrix}
cos\theta  & sin\theta \\
-sin\theta & cos\theta \\
\end{pmatrix}
\begin{pmatrix}
v_1 \\
v_2 \\
\end{pmatrix}
= 
\begin{pmatrix}
v_1cos\theta  + v_2sin\theta \\
-v_1sin\theta + v_2cos\theta \\
\end{pmatrix}
= \vec{v'}
\end{equation}
The vector $\vec{v'}$ is still a vector, it's just $\vec{v}$ rotated by $\theta$. We can write the matrix multiplication in summation form like so $v'_i = \sum_j R_{ij}v_j$. We can also write the transformation in terms of dot products $v'_i = \vec{R_i} \cdot \vec{v}$. 

We can use the dot product form to extend this to functions, $g(x) = K(x) \cdot f$. Writing out the dot product in terms of an integral yields $\int K(x,y)f(y)dy$. Integrating f(y) against the 2D function K(x,y) performs the matrix multiplication yielding the new function g(x). Let's write $Kf = g$ to represent this operation. Imagine the discretized matrix multiplication,
\begin{equation}
\begin{pmatrix}
K(y_1,x_1) & K(y_1, x_2) & ... & K(y_1, x_N) \\
K(y_2,x_1) & K(y_2, x_2) & ... & K(y_2, x_N) \\
... & ... & ... & ... \\
K(y_N,x_1) & K(y_N, x_2) & ... & K(y_N, x_N) \\
\end{pmatrix}
\begin{pmatrix}
f(x_1) \\
f(x_2) \\
... \\ 
f(x_N) \\
\end{pmatrix}
=
\begin{pmatrix}
K(y_1,x_1)f(x_1)dx + K(y_1, x_2)f(x_2)dx + ... + K(y_1, x_N)f(x_N) \\
K(y_2,x_1)f(x_1)dx + K(y_2, x_2)f(x_2)dx + ... + K(y_2, x_N)f(x_N) \\
... + ... + ... + ... \\
K(y_N,x_1)f(x_1)dx + K(y_N, x_2)f(x_2)dx + ... + K(y_N, x_N)f(x_N) \\
\end{pmatrix}
=
\begin{pmatrix}
g(y_1) \\
g(y_2) \\
...    \\
g(y_N) \\
\end{pmatrix}.
\end{equation}
An example is the Fourier transform,
\begin{equation}
F(k,x)f(x) = \frac{1}{2\pi}\int e^{-ikx}f(x)dx = g(k). 
\end{equation} 

%%%%%%%%%%%%%%%%%%%% DERIVATIVE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Derivative As A Matrix}
The derivative also transforms a function into a different function, $\frac{d}{dx}f(x) = \frac{f(x+dx) - f(x)}{dx} = f'(x)$. Let's write $\frac{d}{dx}f(x)$ as a matrix multiplication $Df = \int D(y,x)f(x)dx = g(y)$,
\begin{equation}
\begin{split}
Df &= \frac{1}{dxdx} 
\begin{pmatrix}
... & ... & ... & ... & ... & ... & ... & ... \\
... & -1 & 1 & 0 & 0 & 0 & 0 & ...\\
... & 0 & -1 & 1 & 0 & 0 & 0 & ... \\
... & 0 & 0 & -1 & 1 & 0 & 0 & ... \\
... & 0 & 0 & 0 & -1 & 1 & 0 & ... \\
... & ... & ... & ... & ... & ... & ... & ... \\
\end{pmatrix}
\begin{pmatrix}
... \\
f(x_i) \\
f(x_{i+1}) \\
f(x_{i+2}) \\
f(x_{i+3}) \\
f(x_{i+4}) \\
f(x_{i+5}) \\
... \\
\end{pmatrix}
=
\begin{pmatrix}
... \\
\frac{-f(x_i) + f(x_{i+1})}{dxdx}dx \\
\frac{-f(x_{i+1}) + f(x_{i+2})}{dxdx}dx \\
\frac{-f(x_{i+2}) + f(x_{i+3})}{dxdx}dx \\
\frac{-f(x_{i+3}) + f(x_{i+4})}{dxdx}dx \\
\frac{-f(x_{i+4}) + f(x_{i+5})}{dxdx}dx \\
\frac{-f(x_{i+5}) + f(x_{i+6})}{dxdx}dx \\
... \\
\end{pmatrix}\\
&=
\begin{pmatrix}
... \\
\frac{f(x_{i+1}) - f(x_i) }{dx} \\
\frac{f(x_{i+2}) - f(x_{i+1})}{dx} \\
\frac{f(x_{i+3}) - f(x_{i+2})}{dx} \\
\frac{f(x_{i+4}) - f(x_{i+3})}{dx} \\
\frac{f(x_{i+5}) - f(x_{i+4})}{dx} \\
\frac{f(x_{i+6}) - f(x_{i+5})}{dx} \\
... \\
\end{pmatrix}
=
\begin{pmatrix}
... \\
g(y_i) \\
g(y_{i+1}) \\
g(y_{i+2}) \\
g(y_{i+3}) \\
g(y_{i+4}) \\
g(y_{i+6}) \\
... \\
\end{pmatrix}
\end{split}
\end{equation}
Note that an integral multiplies by a factor of dx, and a derivative divides by dx. We need $\frac{d}{dx}f = \int D(y,x)f(x)dx$ with dimensions like $\frac{1}{dx} \sim D dx$; therefore, $D \sim \frac{1}{dxdx}$. 

%%%%%%%%%%%%%%%%%%%% THE FUNDAMENTAL THEOREM OF CALCULUS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Fundamental Theorem of Calculus}

What happens if we integrate $\frac{d}{dx}f(x)$?

\begin{equation}
\begin{split}
\int_a^b f'(x)dx &= 1 \cdot f'(x) = 
dx
\begin{pmatrix}
... & 1 & 1 & 1 & 1 & 1 & 1 & ... 
\end{pmatrix}
\begin{pmatrix}
... \\
\frac{f(x_{i+1}) - f(x_i) }{dx} \\
\frac{f(x_{i+2}) - f(x_{i+1})}{dx} \\
\frac{f(x_{i+3}) - f(x_{i+2})}{dx} \\
\frac{f(x_{i+4}) - f(x_{i+3})}{dx} \\
\frac{f(x_{i+5}) - f(x_{i+4})}{dx} \\
\frac{f(x_{i+6}) - f(x_{i+5})}{dx} \\
... \\
\end{pmatrix}\\
&= f(b) + ... + f(x_{i+6}) - f(x_{i+5}) + f(x_{i+5}) - f(x_{i+4}) + f(x_{i+4}) - f(x_{i+3}) + ... - f(a) \\ 
&= f(b) - f(a) 
\end{split}
\end{equation}
Boom! The fundamental theorem of calculus. 

%%%%%%%%%%%%%%%%%%%% GREEN FUNCTIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{GREEN FUNCTIONS}
\subsection{Solving A Linear Algebra Problem}
With a linear algebra problem like $M\vec{v} = \vec{b}$, we can solve for $\vec{v}$ by finding $M^{-1}$. With $M^{-1}$ in hand, $\vec{v} = M^{-1}\vec{b}$. Problem solved. But how do we find $M^{-1}$? Well, $M^{-1}$ is easy to find when $M$ is diagonal. Let's go to the bar basis where M is diagonal.

\begin{equation}
\label{minv}
\begin{split}
\bar{M}^{-1}\bar{M} &= 1 = 
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & 1/m_i & 0 & 0 & ... \\
... & 0 & 1/m_{i+1} & 0 & ... \\
... & 0 & 0 & 1/m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix}
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & m_i & 0 & 0 & ... \\
... & 0 & m_{i+1} & 0 & ... \\
... & 0 & 0 & m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix} \\
&\rightarrow \bar{M}^{-1} =  
\begin{pmatrix}
... & ... & ... & ... & ... \\
... & 1/m_i & 0 & 0 & ... \\
... & 0 & 1/m_{i+1} & 0 & ... \\
... & 0 & 0 & 1/m_{i+2} & ... \\
... & ... & ... & ... & ... \\
\end{pmatrix}
\end{split}
\end{equation}
In the bar basis, $\bar{M}$ consists of its eigenvalues along the diagonal, and $\bar{M}^{-1}$ is just one over the eigenvalues along the diagonal. The equation to solve becomes $\vec{\bar{v}} = \bar{M}^{-1}\vec{\bar{b}}$. If we assume there is some transformation matrix T that transforms the non-bar basis to the bar basis, then $T\vec{v} = \bar{M}^{-1}T\vec{b}$ and $\vec{v} = T^{-1}\bar{M}^{-1}T\vec{b}$. Finally, 
\begin{equation}
M^{-1} = T^{-1}\bar{M}^{-1}T.   
\end{equation}
Almost there, we still have to find $T$ and $T^{-1}$! $\vec{\bar{m}}_{i}$, the eigenvector in the bar basis corresponding to eigenvalue $m_i$, is just $\begin{pmatrix}... & 0 & 1 & 0 ... \end{pmatrix}$, where the 1 is in the $i^{th}$ position. $T^{-1}$ converts from the bar basis to the non-bar basis and should convert $\vec{\bar{m}}_{i}$ to $\vec{m}_{i}$. This holds for all eigenvectors. Naturally,
\begin{equation}
\label{tinv}
T^{-1}\vec{\bar{m}}_{i} = \vec{m}_{i} \rightarrow T^{-1} = 
\left( \begin{array}{c|c|c|c|c|c}
   \vec{m}_1 & \vec{m}_2 & ... & \vec{m}_i & ... & \vec{m}_N 
\end{array}\right)
\end{equation} 
Note that $\vec{m}_i$ defines the entire $i^{th}$ column of the $T^{-1}$ matrix. Checking this,
\begin{equation}
T^{-1}\vec{\bar{m}}_{i} = 
\left( \begin{array}{c|c|c|c|c|c}
   \vec{m}_1 & \vec{m}_2 & ... & \vec{m}_i & ... & \vec{m}_N 
\end{array}\right)
\begin{pmatrix}
0 \\ 0 \\ ... \\ 1 \\ ... \\ 0 \\ 
\end{pmatrix}
= 0\vec{v}_{1} + 0 \vec{v}_{2} + ... + \vec{v}_{i} + ... + 0\vec{v}_{N} = \vec{v}_{i}.
\end{equation} 
Our transformation consists of orthonormal eigenvectors which implies that $T^{\dagger}T = 1$. Note that $\dagger$ represents the Hermitian conjugate. 
\begin{equation}
T^{\dagger}T =  
\begin{pmatrix}
\vec{v}_1^\dagger \vec{v}_1 & \vec{v}_1^\dagger \vec{v}_2 & ... & \vec{v}_1^\dagger \vec{v}_N \\
\vec{v}_2^\dagger \vec{v}_1 & \vec{v}_2^\dagger \vec{v}_2 & ... & \vec{v}_2^\dagger \vec{v}_N \\
... & ... & ... & ... \\
\vec{v}_N^\dagger \vec{v}_1 & \vec{v}_N^\dagger \vec{v}_2 & ... & \vec{v}_N^\dagger \vec{v}_N \\
\end{pmatrix}
= 1
\end{equation}
$T^{\dagger}T = 1$ further implies that $T^{-1} = T^{\dagger}$ and $T = T^{-1\dagger}$. We now have the entire solution in hand. $M^{-1} = T^{\dagger}\bar{M}T$ with $\bar{M}$ given by one over the eigenvalues of $M$ (Equation \ref{minv}) and $T^{-1} = T^\dagger$ given by the eigenvectors of $M$ (Equation \ref{tinv}).

Here's an example to solidify things. 

DO A SIMPLE 2x2 EXAMPLE

\subsection{The Green Function: The Inverse Of A Differential Operator}

%%%%%%%%%%%%%%%%%%%% END OF DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
